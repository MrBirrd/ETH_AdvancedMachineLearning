{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otdhQ0nsGRgb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vif calculation\n",
        "def calc_vif(X):\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(\n",
        "        X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)\n",
        "\n",
        "# delete correlated columns\n",
        "def correlation_remover(dataset, threshold):\n",
        "    reduced = dataset.copy()\n",
        "    col_corr = set()\n",
        "    corr_matrix = reduced.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if (abs(corr_matrix.iloc[i, j]) >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "                if colname in reduced.columns:\n",
        "                    del reduced[colname]\n",
        "    return reduced, col_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "t6ULy65IKK39",
        "outputId": "31facb29-022c-4d6a-c647-931c1f0ce226"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('X_train.csv')\n",
        "y = pd.read_csv('y_train.csv')\n",
        "df = pd.concat([y, X], axis=1)\n",
        "\n",
        "# trop the id columns\n",
        "X = X.drop(columns='id')\n",
        "y = y.drop(columns='id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the data was artificially augmented and altered, there are all-zero columns and also duplicates in the data.\n",
        "Those were found by using simple pandas funtions.\n",
        "Median imputing was found to work best here and it's a lot faster than using knn-imputing on this dataset, where we have a very high dimensional feature-space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop zero cols\n",
        "X_selection = X.drop(columns=['x193', 'x297', 'x339', 'x629'])\n",
        "# drop duplicates\n",
        "X_selection = X_selection.drop_duplicates()\n",
        "indexes = X_selection.columns\n",
        "# imputing\n",
        "imp = SimpleImputer(strategy='median').fit(X_selection)\n",
        "X_selection = pd.DataFrame(imp.transform(X_selection), columns=indexes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feature selection is actually rather simple and there could be many other possibilities like ANOVA etc. to select the best features.\n",
        "It is known that the Dataset originally has only about 200 features, but the Data now contains 800 so we have to find those which were artificially added.\n",
        "One could use feature importance of RandomForest or the coefficients of Lasso, but those methods performed worse than the very simple removal of data which correlates\n",
        "only weakly with the target. To find a good threshold, model performance of the out of the box Sklearn models was calculated while changing the threshold.\n",
        "The results are plotted below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](correlation_analysis/corr_vs_score_knn.png)\n",
        "![](correlation_analysis/corr_vs_score_lasso.png)\n",
        "![](correlation_analysis/corr_vs_score_sgd.png)\n",
        "![](correlation_analysis/corr_vs_score_svr.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most algorithms had a peak around correlation threshold of 0.1, expect KNN. Hence, a value around 0.1 was chosen for feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the correlation in respect to y and select only the ones with a certain threshold\n",
        "corr_scores = []\n",
        "\n",
        "corr_y = []\n",
        "high_corr_idx = []\n",
        "\n",
        "for idx, col in enumerate(X_selection.columns):\n",
        "    corr_y.append(\n",
        "        [idx, pd.concat([y, X_selection[col]], axis=1).corr().iloc[0, 1]])\n",
        "\n",
        "for item in corr_y:\n",
        "    if abs(item[1]) >= 0.1:\n",
        "        high_corr_idx.append(item[0])\n",
        "\n",
        "print(\"Selected\", len(high_corr_idx), \"features.\")\n",
        "X_selection = X_selection.iloc[:, high_corr_idx]\n",
        "X_selection, removed_cols = correlation_remover(X_selection, 0.98)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can watch at the VIF values, which look okay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vif = calc_vif(X_selection)\n",
        "vif.sort_values('VIF', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is split into 80% train and 20% test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_selection, y.y, test_size=0.2, random_state=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model is composed of SVR, GBR and Ridge, stacked by the sklearn StackingRegressor.\n",
        "The hyperparameters are found with a GridSearch. Note that the initial grids were a lot bigger and only the best ranges are kept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The support vector regressor was found to perform very well with the standard RBF kernel and mean-variance scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = [\n",
        "    {'svr__C': np.logspace(0, 3, 4),\n",
        "     'svr__epsilon': np.logspace(-6, 0, 7),\n",
        "     'svr__gamma': ['auto', 'scale']},\n",
        "]\n",
        "svr_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svr', SVR())\n",
        "])\n",
        "\n",
        "svr_model = GridSearchCV(svr_model, param_grid,scoring='r2', cv=5, n_jobs=-1)\n",
        "svr_model.fit(X_train, y_train)\n",
        "print(svr_model.best_params_)\n",
        "svr_model = svr_model.best_estimator_\n",
        "print(r2_score(y_test, svr_model.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GBR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient boost regressor adds another approach and should make the total model more robust.\n",
        "The quantile scaler was used, with adapted quantile sizes to fit the dataset better.\n",
        "This was found to work better than the StandardScaler, probably because boosting is known to be sensitive to outliers.\n",
        "The quantile size value was found empirically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_val = 5\n",
        "n_qantiles_full = int(X_train.shape[0]*2/cv_val)\n",
        "\n",
        "param_grid = [{\n",
        "    'gb__n_estimators': [1000],\n",
        "    'gb__min_samples_split': [2, 3],\n",
        "    'gb__min_samples_leaf': [2, 3],\n",
        "    'gb__learning_rate': [0.1],\n",
        "    'gb__max_depth': [3,4],\n",
        "    'gb__max_features': ['sqrt'],\n",
        "    'scaler__n_quantiles': [int(n_qantiles_full/2), n_qantiles_full]\n",
        "}]\n",
        "\n",
        "gb_model = Pipeline([\n",
        "    ('scaler', QuantileTransformer(output_distribution=\"normal\")),\n",
        "    ('gb', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "search = GridSearchCV(gb_model, param_grid, cv=cv_val, n_jobs=-1)\n",
        "search.fit(X_train, y_train)\n",
        "print(search.best_params_)\n",
        "gb_model = search.best_estimator_\n",
        "\n",
        "score = r2_score(y_test, gb_model.predict(X_test))\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = [{\n",
        "    'estimator__n_estimators': [1000],\n",
        "    'estimator__max_features': ['auto', 'log2', 'sqrt']\n",
        "}]\n",
        "\n",
        "adb_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('estimator', ExtraTreesRegressor())\n",
        "])\n",
        "\n",
        "search = GridSearchCV(adb_model, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "search.fit(X_train, y_train)\n",
        "print(search.best_params_)\n",
        "adb_model = search.best_estimator_\n",
        "\n",
        "score = r2_score(y_test, adb_model.predict(X_test))\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An ensemble should make the model more robust and perform better in general. They are also very often amongst the top models in Kaggle.\n",
        "Before fitting the final Regressor, the models are trained on the whole available data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svr_model.fit(X_selection, y.y)\n",
        "gb_model.fit(X_selection, y.y)\n",
        "adb_model.fit(X_selection,y.y)\n",
        "estimators = [\n",
        "    ('knn', adb_model),\n",
        "    ('svr', svr_model),\n",
        "    ('gb', gb_model)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting the stacking regression model with LassoCV, which has built-in cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_pipeline = Pipeline([\n",
        "    ('model', LassoCV())\n",
        "])\n",
        "\n",
        "reg = StackingRegressor(estimators, final_pipeline, n_jobs=-1)\n",
        "reg.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score = r2_score(y_test, reg.predict(X_test))\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As one can see, the ensemble model performs better than the individual models and was found to be more robust too.\n",
        "Before predicting on test data, the model should be trained on the whole available data. Then the test data must be processed in the same way."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Task1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8d978ae2e8a86257f048fc033d3eda8d42f083b4862912b2e1169677d4916a1a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
