{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.svm import OneClassSVM\n",
    "import xgboost as xgb\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import specialised modules\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are calculated for the whole data. This only shows how it was done, the results are already saved as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = False\n",
    "\n",
    "if new_features:\n",
    "    # prepare the X data for analysis\n",
    "    X_ = pd.read_csv('X_train.csv', engine='c')\n",
    "    X_.drop(columns='id', inplace=True)\n",
    "    col_names = X_.index\n",
    "    # transform the data\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_processed = pd.DataFrame(scaler.fit_transform(X_.transpose()).transpose())\n",
    "    # convert the frame to np arrays and remove the nans\n",
    "    X_processed = [item[~np.isnan(item)] for item in X_processed.to_numpy()]\n",
    "    #extracting noise features\n",
    "    noise_df = extract_noise_features(X_processed)\n",
    "    X_pp = preprocess(X_processed)\n",
    "    #extract features based on temporal and frequental things\n",
    "    df = extract_features(X_pp)\n",
    "    df = pd.concat([df, noise_df], axis=1)\n",
    "    df.to_csv('../data/features.csv')\n",
    "else:\n",
    "    df  = pd.read_csv('features.csv').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print general description\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Imputing was used for imputing missing data. Sometimes the libraries failes, especially in the noisy class, since the ECG libraries try to find\n",
    "PQRST templates nonthereless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace inf with nan\n",
    "df.replace([np.inf, -np.inf], np.NaN, inplace=True)\n",
    "imp = KNNImputer(n_neighbors=4, weights='distance')\n",
    "df_X = pd.DataFrame(imp.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('y_train.csv')\n",
    "y.drop(columns='id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be the further processing of the time-series data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# making masks for extracting values per class\n",
    "mask0 = [y==0][0].to_numpy()\n",
    "mask1 = [y==1][0].to_numpy()\n",
    "mask2 = [y==2][0].to_numpy()\n",
    "mask3 = [y==3][0].to_numpy()\n",
    "\n",
    "#apply masks\n",
    "X_0 = [i for (i, v) in zip(X_processed, mask0) if v]\n",
    "X_1 = [i for (i, v) in zip(X_processed, mask1) if v]\n",
    "X_2 = [i for (i, v) in zip(X_processed, mask2) if v]\n",
    "X_3 = [i for (i, v) in zip(X_processed, mask3) if v]\n",
    "\n",
    "#preprocess (remove baseline wander, filter and centering y to 0)\n",
    "X_0pp = preprocess(X_0)\n",
    "X_1pp = preprocess(X_1)\n",
    "X_2pp = preprocess(X_2)\n",
    "X_3pp = preprocess(X_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers with a one class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_before = df_X.shape[0]\n",
    "\n",
    "trans = ExperimentalTransformer(OneClassSVM(nu=0.995))\n",
    "trans.fit(df_X, y)\n",
    "\n",
    "X_selection, y = trans.transform(df_X, y)\n",
    "features_after = X_selection.shape[0]\n",
    "\n",
    "print(f'Data size reduced from {features_before} to {features_after}')\n",
    "df_X = X_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most important features with a random forest, since different libraries are used and they showed very inconsistant performance.\n",
    "The 40 best features are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance (optional)\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=200, n_jobs=-1), n_features_to_select=40, step=1)\n",
    "rfe.fit(df_X, y.values.ravel())\n",
    "ranking = rfe.ranking_\n",
    "ranked = pd.DataFrame(columns=['rank','feature'])\n",
    "ranked['feature']= df.columns\n",
    "ranked['rank'] = ranking\n",
    "rank = ranked.sort_values('rank')\n",
    "df_X = rfe.transform(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_selection, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB is often used for tabular data for many reasons and it's potential was again showed in the recent paper [Tabular Data: Deep Learning is Not All You Need](https://arxiv.org/abs/2106.03253).\n",
    "\n",
    "The hyperopt package is used to maximize the AUC while finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "space = {'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "         'gamma': hp.uniform('gamma', 1, 9),\n",
    "         'reg_alpha': hp.quniform('reg_alpha', 40, 180, 1),\n",
    "         'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
    "         'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "         'min_child_weight': hp.quniform('min_child_weight', 0, 10, 1),\n",
    "         'n_estimators': 1000,\n",
    "         'seed': 0\n",
    "         }\n",
    "\n",
    "def objective(space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        use_label_encoder=False,n_jobs=-1,\n",
    "        n_estimators=space['n_estimators'], max_depth=int(space['max_depth']), gamma=space['gamma'],\n",
    "        reg_alpha=int(space['reg_alpha']), min_child_weight=int(space['min_child_weight']),\n",
    "        colsample_bytree=int(space['colsample_bytree']))\n",
    "\n",
    "    evaluation = [(X_train, y_train.values.ravel()),\n",
    "                  (X_test, y_test.values.ravel())]\n",
    "\n",
    "    clf.fit(X_train, y_train.values.ravel(),\n",
    "            eval_set=evaluation, eval_metric=\"auc\",\n",
    "            early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = f1_score(y_test.values.ravel(), pred, average='micro')\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 500,\n",
    "                        trials = trials)\n",
    "\n",
    "best_xgb = xgb.XGBClassifier(best_hyperparams)\n",
    "best_xgb.fit(X_train, y_train.values.ravel(), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, best_xgb.predict(X_test), average=None))\n",
    "print(f1_score(y_test, best_xgb.predict(X_test), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the model on the submission, it was finetuned on the whole data, since we should make use of all data we have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d978ae2e8a86257f048fc033d3eda8d42f083b4862912b2e1169677d4916a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
