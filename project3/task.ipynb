{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import cv2\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Lambda\n",
    "from keras.layers import Conv2DTranspose, Concatenate, Input, GlobalAveragePooling2D, Multiply, Reshape, Dense, Add\n",
    "from keras.models import Model\n",
    "from keras.backend import int_shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.filters import threshold_yen, threshold_multiotsu\n",
    "from skimage.restoration import denoise_nl_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function just loads a zipped pickle file and unpacks it\n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "# this function saves an object as a zipped pickle file\n",
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)\n",
    "\n",
    "# rescaling the image to size (x,y) by cubic interpolation\n",
    "def rescale_image(img, x, y):\n",
    "    return cv2.resize(img, dsize=(x, y), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# blur the image with either a median or a gaussian kernel, median was\n",
    "# found to perform better\n",
    "def blur_image(img, mode=\"median\"):\n",
    "  if mode == \"median\":\n",
    "    blurred = cv2.medianBlur(img,3)\n",
    "  if mode == \"gauss\":\n",
    "    blurred = cv2.GaussianBlur(img, (3,3), 1)\n",
    "\n",
    "  return blurred\n",
    "\n",
    "def denoise_img(img, rep=1):\n",
    "  if rep > 1:\n",
    "    for i in range(rep):\n",
    "      img = denoise_nl_means(img)\n",
    "  else:\n",
    "    img = denoise_nl_means(img)\n",
    "\n",
    "  return img\n",
    "\n",
    "# add a layer with threshold \n",
    "def produce_mask_layer(array, method='mo'):\n",
    "  regions = []\n",
    "  for img in array:\n",
    "    if method == 'mo':\n",
    "      thresholds = threshold_multiotsu(img, classes=3)\n",
    "      regions.append(np.digitize(img, bins=thresholds))\n",
    "    if method == 'yen':\n",
    "      threshold = threshold_yen(img)   \n",
    "      regions.append(np.array(img > threshold, dtype='int'))\n",
    "      \n",
    "  return regions\n",
    "\n",
    "# the idea is to add 2 channels, one containing a blurred layer and one with\n",
    "# a threshold layer, the medianblur was found to work good. The thresholding\n",
    "# may be still adapted\n",
    "def preprocess_data(dataset, method='simple'):\n",
    "\n",
    "  if method == 'simple':\n",
    "    return np.stack([dataset, dataset, dataset], axis=-1)\n",
    "\n",
    "  if method == 'denoise_multiotsu':\n",
    "    img = [denoise_img(img, 3) for img in dataset]\n",
    "    blurred = [blur_image(img) for img in dataset]\n",
    "    thresh = produce_mask_layer(img, 'mo')\n",
    "    return np.stack([dataset, blurred, thresh], axis=-1)\n",
    "  \n",
    "  if method == 'denoise_yen':\n",
    "    img = [denoise_img(img, 3) for img in dataset]\n",
    "    blurred = [blur_image(img) for img in dataset]\n",
    "    thresh = produce_mask_layer(img, 'yen')\n",
    "    return np.stack([dataset, blurred, thresh], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of filters of the first layer in the unet. \n",
    "# it automatically gets doubled every deeper layer\n",
    "n_filts = 64\n",
    "\n",
    "# defining the image size, 128x128 was found to be quick and works very well\n",
    "# as one can see. The size does not really affect the mask output, there\n",
    "# were papers which found different ratios have not much impact to the output \n",
    "# mask result, even if they have a different ratio than 1:1\n",
    "im_size = [128,128]\n",
    "\n",
    "# the size is 128x128x3 because we get 3 layers\n",
    "# instead of rgb we have normal,blurred,threshold\n",
    "input_shape = (im_size[0], im_size[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# as suggested in https://arxiv.org/pdf/2006.04868.pdf, in usage of the double\n",
    "# unet we use a squeeze excite block (need to visualize what it exactly does)\n",
    "# the double unet didn't really work with vgg as encoder, so only the squeez\n",
    "# excite block is kept at the moment.\n",
    "#\n",
    "# it squeezes the input which is a conv2d output, puts it into a dense layer\n",
    "# with nfilts/8 and the expands it again. This squeeze_multiply is multiplicated\n",
    "# with the input\n",
    "def squeeze_excite_block(inputs, ratio=16):\n",
    "    init = inputs\n",
    "    channel_axis = -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    x = Multiply()([init, se])\n",
    "    return x\n",
    "\n",
    "\n",
    "# lambda function to repeat Repeats the elements of a tensor along an axis\n",
    "# by a factor of rep.\n",
    "# If tensor has shape (None, 256,256,3), lambda will return a tensor of shape \n",
    "# (None, 256,256,6), if specified axis=3 and rep=2.\n",
    "def repeat_elem(tensor, rep):\n",
    "     return Lambda(lambda x, repnum: keras.backend.repeat_elements(x, repnum, axis=3),\n",
    "                          arguments={'repnum': rep})(tensor)\n",
    "\n",
    "\n",
    "\n",
    "# resize the down layer feature map into the same dimension as the up layer feature map\n",
    "# using 1x1 conv\n",
    "# :return: the gating feature map with the same dimension of the up layer feature map\n",
    "def gating_signal(input, out_size):\n",
    "\n",
    "    x = Conv2D(out_size, (1, 1), padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# the convolution block consists of 2 layers of 2D concolution and is\n",
    "# used in every layer of the encoder and decoder\n",
    "def conv_block(input, num_filters, size=3, dropout=0):\n",
    "    # first block, it's a 2d convolution with depth 3\n",
    "    # the input is given by the num_filters variable, to connect it\n",
    "    # to other layers of unet\n",
    "    x = Conv2D(num_filters, size, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # second block, which is the same as before, but it takes the first block\n",
    "    # output as input\n",
    "    x = Conv2D(num_filters, size, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    if dropout > 0:\n",
    "      x = Dropout(dropout)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# residual convolution from\n",
    "# https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
    "def res_conv_block(input, num_filters, size=3, dropout=0):\n",
    "    # first block, it's a 2d convolution with depth 3\n",
    "    # the input is given by the num_filters variable, to connect it\n",
    "    # to other layers of unet\n",
    "    x = Conv2D(num_filters, (size, size), padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # second block, which is the same as before, but it takes the first block\n",
    "    # output as input\n",
    "    x = Conv2D(num_filters, (size, size), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Activation(\"relu\")(x) # no activation yet\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    if dropout > 0:\n",
    "      x = Dropout(dropout)(x)\n",
    "    \n",
    "    shortcut = Conv2D(num_filters, kernel_size=(1, 1), padding=\"same\")(input)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    residual = Add()([shortcut, x])\n",
    "    residual = Activation(\"relu\")(residual)\n",
    "    return residual\n",
    "\n",
    "def attention_block(x, gating, inter_shape):\n",
    "  shape_x = int_shape(x)\n",
    "  shape_g = int_shape(gating)\n",
    "\n",
    "  theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding=\"same\")(x)\n",
    "  shape_theta_x = int_shape(theta_x)\n",
    "\n",
    "  phi_g = Conv2D(inter_shape, (1, 1), padding=\"same\")(gating)\n",
    "  g_upsampled = Conv2DTranspose(inter_shape, (3, 3), strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]), padding=\"same\")(phi_g)\n",
    "\n",
    "  xg_concat = Add()([g_upsampled, theta_x])\n",
    "  xg_act = Activation('relu')(xg_concat)\n",
    "  psi = Conv2D(1, (1, 1), padding='same')(xg_act)\n",
    "  xg_sigmoid = Activation('sigmoid')(psi)\n",
    "  shape_sigmoid = int_shape(xg_sigmoid)\n",
    "  upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(xg_sigmoid)  # \n",
    "  \n",
    "  upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
    "  y = Multiply()([upsample_psi, x])\n",
    "\n",
    "  result = Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
    "  result_bn = BatchNormalization()(result)\n",
    "  return result_bn\n",
    "\n",
    "# the encoder block takes the convolution block and adds maxpooling\n",
    "# to connect to the next layer\n",
    "def encoder_block(input, num_filters, args):\n",
    "    # unpack the arguments\n",
    "    res, att, drop = args\n",
    "    # calling the convolution block to convolve the input with 2 layers\n",
    "    # of conv2d\n",
    "    if res:\n",
    "      x = res_conv_block(input, num_filters, dropout=drop)\n",
    "    else:\n",
    "      x = conv_block(input, num_filters, dropout=drop)\n",
    "    # maxpooling to prepare the output for the next layer which doubles the\n",
    "    # filter size\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "# the decoder makes the up convolution and then convolution again\n",
    "def decoder_block(input, skip_features, num_filters, args):\n",
    "    res, att, drop = args\n",
    "    \n",
    "    # do the soft attention\n",
    "    if att:\n",
    "      gate_ = gating_signal(input, num_filters)\n",
    "      att_  = attention_block(skip_features, gate_, num_filters)\n",
    "      x = UpSampling2D(size= (2,2))(input)\n",
    "      x = Concatenate()([x, att_])\n",
    "    else:\n",
    "      x = UpSampling2D(size= (2,2), data_format=\"channels_last\")(input)\n",
    "      x = Concatenate()([x, skip_features])\n",
    "\n",
    "    # do convolution 2 times\n",
    "    if res:\n",
    "      x = res_conv_block(x, num_filters, dropout=drop)\n",
    "    else:\n",
    "      x = conv_block(x, num_filters, dropout=drop)\n",
    "\n",
    "    return x\n",
    "\n",
    "# defining the unet\n",
    "def build_unet(input_shape, res=False, att=False, drop=0):\n",
    "    inputs = Input(input_shape, dtype=tensorflow.float32)\n",
    "    in_bn = BatchNormalization()(inputs)\n",
    "    args = (res, att, drop)\n",
    "    # layers down, each layer gives the direct output s for usage as\n",
    "    # inputs to the decoder side directly, an the maxpool outputs p\n",
    "    # for usage in deeper levels\n",
    "    s1, p1 = encoder_block(in_bn, n_filts, args)\n",
    "    s2, p2 = encoder_block(p1, n_filts*2, args)\n",
    "    s3, p3 = encoder_block(p2, n_filts*4, args)\n",
    "    s4, p4 = encoder_block(p3, n_filts*8, args)\n",
    "    # bridge block, it makes 2 convolutions on the lowest level, no max pooling\n",
    "    # since the output gous up again which is handled by the conv2dtranspose\n",
    "    if res:\n",
    "      b1 = res_conv_block(p4, n_filts*16, dropout=drop)\n",
    "    else:\n",
    "      b1 = conv_block(p4, n_filts*16, dropout=drop)\n",
    "    # layers up, each layer has just one output to go into transpose of the\n",
    "    # upper layer. Each decoder takes the output of the lower level (s) and the \n",
    "    # output of the encoder at the same level (d)\n",
    "    d1 = decoder_block(b1, s4, n_filts*8, args)\n",
    "    d2 = decoder_block(d1, s3, n_filts*4, args)\n",
    "    d3 = decoder_block(d2, s2, n_filts*2, args)\n",
    "    d4 = decoder_block(d3, s1, n_filts, args)\n",
    "    # output layer\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "    model = Model(inputs, outputs, name=\"U-net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = np.load('sample.npy', allow_pickle=True)[()]\n",
    "test_data = np.load('sample.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale down the video data\n",
    "train_data['video'] = train_data['video'].astype('float32') / 255.0\n",
    "test_data['video'] = test_data['video'].astype('float32') / 255.0\n",
    "\n",
    "# artificially augment\n",
    "train_data = [train_data] * 10\n",
    "test_data = [test_data] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the Xtrain and ytrain arrays\n",
    "\n",
    "X_train_full = []\n",
    "y_train_full = []\n",
    "boxes_train_full = []\n",
    "\n",
    "# extract the usable data, this just adds each frame, not caring about\n",
    "# from which patient it comes\n",
    "for item in train_data:\n",
    "    frames = item['frames']\n",
    "    # extract the video frames which are annotated\n",
    "    X_train_full.append(item['video'][:,:,frames[0]])\n",
    "    X_train_full.append(item['video'][:,:,frames[1]])\n",
    "    X_train_full.append(item['video'][:,:,frames[2]])\n",
    "    # extract the labels for classification\n",
    "    y_train_full.append(item['label'][:,:,frames[0]])\n",
    "    y_train_full.append(item['label'][:,:,frames[1]])\n",
    "    y_train_full.append(item['label'][:,:,frames[2]])\n",
    "    # extract the boxes, to match the other data, extract it 3 times\n",
    "    boxes_train_full.append(item['box'])\n",
    "    boxes_train_full.append(item['box'])\n",
    "    boxes_train_full.append(item['box'])\n",
    "\n",
    "# this converts the boolean array into 0 and 1\n",
    "y_train_full = [np.array(i, dtype=np.uint8) for i in y_train_full]\n",
    "boxes_train_full = [np.array(i, dtype=np.uint8) for i in boxes_train_full]\n",
    "\n",
    "# resize the pictures to the defined size\n",
    "for idx, item in enumerate(X_train_full):\n",
    "    X_train_full[idx] = rescale_image(item, im_size[0], im_size[1])\n",
    "\n",
    "for idx, item in enumerate(y_train_full):\n",
    "    y_train_full[idx] = rescale_image(item, im_size[0], im_size[1])\n",
    "\n",
    "for idx, item in enumerate(boxes_train_full):\n",
    "    boxes_train_full[idx] = rescale_image(item, im_size[0], im_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing was a very important step in making this pipeline achieve a good score.\n",
    "The idea is to add additional channels by masks, generated using different threshold methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = preprocess_data(X_train_full, method='denoise_yen')\n",
    "y_train_full = np.expand_dims(y_train_full, -1)\n",
    "boxes_train_full = np.expand_dims(boxes_train_full, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the keras ImageDataGenerator to augment the dataset with shifts, zoom, and shear.\n",
    "This helped with the low amount of data. Note however, that the flipping was deactivated.\n",
    "This is because the model should not start to segment the left mitral valve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generators(batch_size,X_train, X_test, y_train, y_test):\n",
    "  # fix seed for the datagenerators and batch size\n",
    "  seed=24\n",
    "\n",
    "  from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "  # data generators which randomly shear, shift, zoom, flip\n",
    "  # and do more juust in an infinite loop.\n",
    "  # the same generator is applied to the image and the mask\n",
    "  # so the loss is generated correctly\n",
    "  #\n",
    "  # deactivating flipping really also helped boosting the model\n",
    "  # there is no flipping anyways in the testset\n",
    "  img_data_gen_args = dict(rotation_range=60,\n",
    "                      width_shift_range=0.3,\n",
    "                      height_shift_range=0.2,\n",
    "                      shear_range=0.2,\n",
    "                      zoom_range=0.2,\n",
    "                      horizontal_flip=False,\n",
    "                      vertical_flip=False,\n",
    "                      fill_mode='constant'\n",
    "                      )\n",
    "\n",
    "  mask_data_gen_args = dict(rotation_range=60,\n",
    "                      width_shift_range=0.3,\n",
    "                      height_shift_range=0.2,\n",
    "                      shear_range=0.2,\n",
    "                      zoom_range=0.2,\n",
    "                      horizontal_flip=False,\n",
    "                      vertical_flip=False,\n",
    "                      fill_mode='constant',\n",
    "                      preprocessing_function = lambda x: np.where(x > 0, 1, 0).astype(x.dtype))\n",
    "\n",
    "  # initiate the generators, switch from train and test split to internal 0.2 split\n",
    "  # to not retrain the network again after fitting\n",
    "  image_data_generator = ImageDataGenerator(**img_data_gen_args)\n",
    "  image_generator = image_data_generator.flow(X_train, seed=seed, batch_size = batch_size)\n",
    "  valid_img_generator = image_data_generator.flow(X_test, seed=seed, batch_size = batch_size)\n",
    "  mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n",
    "  mask_generator = mask_data_generator.flow(y_train, seed=seed, batch_size = batch_size)\n",
    "  valid_mask_generator = mask_data_generator.flow(y_test, seed=seed, batch_size = batch_size)\n",
    "\n",
    "  # combining the generators to feed it directly as a tuple to the keras model\n",
    "  def image_mask_gemerator(image_generator, mask_generator):\n",
    "      train_generator = zip(image_generator, mask_generator)\n",
    "      for (img, mask) in train_generator:\n",
    "          yield (img, mask)\n",
    "\n",
    "  train_gen = image_mask_gemerator(image_generator, mask_generator)\n",
    "  val_gen = image_mask_gemerator(valid_img_generator, valid_mask_generator)\n",
    "\n",
    "  return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_train_full, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "batch_size = 8\n",
    "steps_per_epoch = 80\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile with adam and binary focal jaccard loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate = lr), loss=sm.losses.binary_focal_jaccard_loss, metrics=[sm.metrics.iou_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, val_generator = build_generators(batch_size, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, validation_data = val_generator, \n",
    "                steps_per_epoch = steps_per_epoch, \n",
    "                validation_steps = steps_per_epoch,\n",
    "                epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction was converted to a binary mask via thresholding. The threshold value was found to be very important, as it also \n",
    "determines the \"sharpness\" of the mask. A high threshold lead to much better scores than just using 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_thresholded = y_pred >= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = np.logical_and(y_test, y_pred_thresholded)\n",
    "union = np.logical_or(y_test, y_pred_thresholded)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "print(\"IoU socre is: \", iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frames were furthermore processed by anisotropic diffusion and erosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example prediction is shown below, where the mask is colored red. The video is in slow-motion (slowed down by factor of 3) to better see the movements.\n",
    "\n",
    "![SegmentLocal](example.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](example.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d978ae2e8a86257f048fc033d3eda8d42f083b4862912b2e1169677d4916a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
